{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d83b9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "class page:\n",
    "    \n",
    "    #Create a master list of stopwords, letters, punctuation, and numbers\n",
    "\n",
    "    excluded_list = []\n",
    "\n",
    "    excluded_list.extend(stopwords.words('english'))\n",
    "    excluded_list.extend(string.punctuation)\n",
    "    excluded_list.extend(string.ascii_lowercase)\n",
    "    extras = [\"–\",\"‘\", \"’\", \"“\", \"”\",\"'s\", \"’s\",'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "                'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen',\n",
    "                'eighteen', 'nineteen', 'twenty', 'thirty', 'fourty', 'fifty', 'sixty', 'seventy',\n",
    "                'eighty', 'ninety', 'hundred', 'thousand', \"n't\", 'wo', 'ca', 'sha',\n",
    "                \"'re\", \"'d\", \"'ll\"]\n",
    "    excluded_list.extend(extras)\n",
    "    excluded_set = set(excluded_list)\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "        #requests function get() along with content pulls the html code of the given website\n",
    "        page = requests.get(url)\n",
    "        cont = page.content\n",
    "\n",
    "        #BeautifulSoup parses the requests content into a form navigable by bs\n",
    "        self.soup = BeautifulSoup(cont, 'html.parser')\n",
    "        \n",
    "\n",
    "    '''\n",
    "    links() extracts all relevant urls within the instance page, relevant means including the \n",
    "    \"allsides.com/news/2024\" string. This string is uniform to links to internal allsides news articles.\n",
    "    '''\n",
    "\n",
    "    def links(self):\n",
    "\n",
    "        links = []\n",
    "        #find_all('a) pulls all article tags <a>\n",
    "        for link in self.soup.find_all('a'):\n",
    "            #link.get('href') pulls the actual url from the 'href' within the article tag <a>\n",
    "            href = str(link.get('href'))\n",
    "            #restrict to news links from allsides in 2024\n",
    "            if 'www.allsides.com/news/2024' in href:\n",
    "                links.append(href)\n",
    "\n",
    "        return list(set(links))\n",
    "    \n",
    "    '''\n",
    "    bias() extracts the bias of the instance page. This is coded numerically. If the page is problematic or \n",
    "    has no bias, the code 888 is returned.\n",
    "    '''\n",
    "    \n",
    "    def bias(self):\n",
    "       \n",
    "        sp_bias = self.soup.find('div', class_='article-media-bias-')\n",
    "        \n",
    "        #An error return option if link is broken\n",
    "        if sp_bias is None:\n",
    "            return 888\n",
    "\n",
    "        #Get the bias from article\n",
    "        bias = sp_bias.find('a').get_text()\n",
    "        \n",
    "        token = 888\n",
    "        if bias == 'Center':\n",
    "            token = 0\n",
    "        if bias == 'Lean Left':\n",
    "            token = -1\n",
    "        if bias == 'Left':\n",
    "            token = -2\n",
    "        if bias == 'Lean Right':\n",
    "            token = 1\n",
    "        if bias == 'Right':\n",
    "            token = 2\n",
    "\n",
    "        return token\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    '''\n",
    "    text() method returns the relevant text in the article, what is relevant can be determined by the option type.\n",
    "    type has two options, default is \"article-description\" for short form article text, alternate is\n",
    "    \"article-name\" for just the title. \n",
    "    '''\n",
    "\n",
    "    def text(self, type = 'article-description'):\n",
    "        \n",
    "        sp_text = self.soup.find('div', class_='article-description')\n",
    "        \n",
    "        #An error return option if link is broken\n",
    "        if sp_text is None:\n",
    "            return 888\n",
    "        \n",
    "        #Get text from article\n",
    "        text_0 = sp_text.get_text()\n",
    "\n",
    "        \n",
    "        text_tokens = word_tokenize(text_0)\n",
    "\n",
    "        ss= SnowballStemmer(language='english')\n",
    "\n",
    "        #Remove stopwords from title.\n",
    "        filtered_text = []\n",
    "        for t in text_tokens:\n",
    "            tl = t.lower()\n",
    "            tlr1 = tl.replace('.', '')\n",
    "            if '-' in tlr1:\n",
    "                t_list_temp = word_tokenize(tlr1.replace('-', ' '))\n",
    "                t_list = [x for x in t_list_temp]\n",
    "            else:\n",
    "                t_list = [tlr1]\n",
    "            for wrd in t_list:\n",
    "                if (not wrd in self.excluded_set) and (wrd.isalpha()):\n",
    "                    filtered_text.append(ss.stem(wrd))\n",
    "\n",
    "        return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f55148e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class iterative_scraper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.All = []\n",
    "\n",
    "    def scrape(self, initial_url, n_steps , p, existing_url_list = []):\n",
    "        pg = page(initial_url)\n",
    "        l = pg.links()\n",
    "\n",
    "        urls = [[x for x in l if x not in existing_url_list]]\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            step_urls = []\n",
    "            \n",
    "            for url in urls[i]:\n",
    "                if np.random.binomial(1, p[i], 1)[0] == 1:\n",
    "                    pg = page(url)\n",
    "                    b = pg.bias()\n",
    "                    t = pg.text()\n",
    "                    l = pg.links()\n",
    "                    if (b != 888) and (t != 888):\n",
    "                        self.All.append([url, b, t])\n",
    "                    step_urls.extend([x for x in l if x not in existing_url_list])\n",
    "                    existing_url_list.extend(step_urls)\n",
    "            urls.append(step_urls)\n",
    "\n",
    "    def to_df(self):\n",
    "        df = pd.DataFrame(self.All, columns=['URL', 'Label', 'Text'])\n",
    "        return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52d0ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3)\n",
      "Empty DataFrame\n",
      "Columns: [URL, Label, Text]\n",
      "Index: []\n",
      "--- 0.18048691749572754 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "compiled_df =pd.read_csv('AllSides_All.csv')\n",
    "\n",
    "exist_urls = compiled_df['URL'].to_list()\n",
    "\n",
    "scpr = iterative_scraper()\n",
    "\n",
    "scpr.scrape(initial_url = 'https://www.allsides.com/unbiased-balanced-news', n_steps =3, p = [1,1,1], existing_url_list = exist_urls)\n",
    "\n",
    "df = scpr.to_df()\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "df_all = compiled_df.append(df)\n",
    "\n",
    "df_all.to_csv('AllSides_All.csv', index= False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65873a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac8be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c929b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
